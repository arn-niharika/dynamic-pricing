{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bus Price Prediction - Dask-Based Full Dataset Training\n",
    "\n",
    "**Improvements over batch training:**\n",
    "1. Proper global shuffle before train/test split\n",
    "2. Single model training (no incremental tree accumulation issues)\n",
    "3. Consistent feature encoding across all data\n",
    "4. Memory-efficient processing with Dask\n",
    "\n",
    "**Expected R²: ~0.70+ (similar to small dataset)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "!pip install dask[dataframe] pyarrow xgboost lightgbm scikit-learn==1.4.2 python-dateutil -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete.\n",
      "Python Libraries Loaded:\n",
      "  - pandas: 2.2.2\n",
      "  - numpy: 2.0.2\n",
      "  - dask: installed\n",
      "  - scikit-learn: 1.4.2\n",
      "  - xgboost: 3.1.3\n",
      "  - lightgbm: 4.6.0\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser as date_parser\n",
    "import logging\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment setup complete.\")\n",
    "print(f\"Python Libraries Loaded:\")\n",
    "print(f\"  - pandas: {pd.__version__}\")\n",
    "print(f\"  - numpy: {np.__version__}\")\n",
    "print(f\"  - dask: {dd.__version__ if hasattr(dd, '__version__') else 'installed'}\")\n",
    "import sklearn\n",
    "print(f\"  - scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"  - xgboost: {xgb.__version__}\")\n",
    "print(f\"  - lightgbm: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PATH DETECTION\n",
      "================================================================================\n",
      "Current working directory: /content\n",
      "Home directory: /root\n",
      "  ✗ /data/complete_data - does not exist\n",
      "  ✗ /content/data/complete_data - does not exist\n",
      "  ✗ /home/sj/Downloads/dynamic-pricing/data/complete_data - does not exist\n",
      "  ✗ /root/Downloads/dynamic-pricing/data/complete_data - does not exist\n",
      "  ✗ /content/data/complete_data - does not exist\n",
      "  ✗ /data/complete_data - does not exist\n",
      "  ✗ /data/complete_data - does not exist\n",
      "\n",
      "⚠️  Could not auto-detect data directory!\n",
      "Please set DATA_DIR manually below:\n",
      "DATA_DIR = '/your/actual/path/to/complete_data'\n",
      "\n",
      "Contents of /content:\n",
      "  .config\n",
      "  sample_data\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                           CONFIGURATION - AUTO PATH DETECTION\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PATH DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Print environment info\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Home directory: {os.path.expanduser('~')}\")\n",
    "\n",
    "# Try to find the data directory by searching common locations\n",
    "possible_paths = [\n",
    "    # Relative paths (if notebook is in notebooks/)\n",
    "    '../data/complete_data',\n",
    "    'data/complete_data',\n",
    "    # Absolute paths\n",
    "    '/home/sj/Downloads/dynamic-pricing/data/complete_data',\n",
    "    os.path.expanduser('~/Downloads/dynamic-pricing/data/complete_data'),\n",
    "    # From current working directory\n",
    "    os.path.join(os.getcwd(), 'data', 'complete_data'),\n",
    "    os.path.join(os.getcwd(), '..', 'data', 'complete_data'),\n",
    "    # Check parent directories\n",
    "    os.path.join(Path(os.getcwd()).parent, 'data', 'complete_data'),\n",
    "]\n",
    "\n",
    "DATA_DIR = None\n",
    "for path in possible_paths:\n",
    "    abs_path = os.path.abspath(path)\n",
    "    exists = os.path.exists(abs_path)\n",
    "    if exists:\n",
    "        parquet_count = len(glob.glob(os.path.join(abs_path, '*.parquet')))\n",
    "        print(f\"  ✓ {abs_path} - {parquet_count} parquet files\")\n",
    "        if parquet_count > 0 and DATA_DIR is None:\n",
    "            DATA_DIR = abs_path\n",
    "    else:\n",
    "        print(f\"  ✗ {abs_path} - does not exist\")\n",
    "\n",
    "if DATA_DIR is None:\n",
    "    print(\"\\n⚠️  Could not auto-detect data directory!\")\n",
    "    print(\"Please set DATA_DIR manually below:\")\n",
    "    print(\"DATA_DIR = '/your/actual/path/to/complete_data'\")\n",
    "    # Try to list what's in current directory\n",
    "    print(f\"\\nContents of {os.getcwd()}:\")\n",
    "    for item in os.listdir(os.getcwd())[:20]:\n",
    "        print(f\"  {item}\")\n",
    "else:\n",
    "    print(f\"\\n✓ Using data directory: {DATA_DIR}\")\n",
    "\n",
    "# Save directory (relative to data dir)\n",
    "if DATA_DIR:\n",
    "    SAVE_DIR = os.path.abspath(os.path.join(DATA_DIR, '..', '..', 'models', 'saved_runs'))\n",
    "else:\n",
    "    SAVE_DIR = './models/saved_runs'\n",
    "\n",
    "CONFIG = {\n",
    "    'data_dir': DATA_DIR,\n",
    "    'save_dir': SAVE_DIR,\n",
    "    'sample_fraction': 1.0,\n",
    "    'min_price': 100,\n",
    "    'max_price': 10000,\n",
    "    'outlier_method': 'iqr_1.5',\n",
    "    'test_size': 0.15,\n",
    "    'shuffle': True,\n",
    "    'models_to_train': ['LightGBM', 'XGBoost'],\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "if DATA_DIR:\n",
    "    os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "    print(f\"\\nConfiguration ready. Found {len(glob.glob(os.path.join(DATA_DIR, '*.parquet')))} parquet files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Data Loading with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data with Dask...\n",
      "================================================================================\n",
      "Using data directory: None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1453717736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Discover all parquet files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mparquet_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Pattern: {parquet_pattern}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    LOAD ALL PARQUET FILES WITH DASK\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Loading data with Dask...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use CONFIG from previous cell\n",
    "data_dir = CONFIG['data_dir']\n",
    "print(f\"Using data directory: {data_dir}\")\n",
    "\n",
    "# Discover all parquet files\n",
    "parquet_pattern = os.path.join(data_dir, '*.parquet')\n",
    "print(f\"Pattern: {parquet_pattern}\")\n",
    "\n",
    "all_files = sorted(glob.glob(parquet_pattern))\n",
    "print(f\"Found {len(all_files)} parquet files\")\n",
    "\n",
    "if not all_files:\n",
    "    # Debug: try listing directory directly\n",
    "    print(\"\\nDEBUG: Listing directory contents...\")\n",
    "    if os.path.exists(data_dir):\n",
    "        contents = os.listdir(data_dir)\n",
    "        print(f\"Directory contains {len(contents)} items\")\n",
    "        print(f\"First 5: {contents[:5]}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {data_dir}\")\n",
    "    raise FileNotFoundError(f\"No parquet files found in {data_dir}\")\n",
    "\n",
    "print(f\"\\nFirst 3 files: {all_files[:3]}\")\n",
    "\n",
    "# Load with Dask (lazy loading - doesn't load into memory yet)\n",
    "ddf = dd.read_parquet(parquet_pattern, engine='pyarrow')\n",
    "\n",
    "print(f\"\\nDask DataFrame created (lazy):\")\n",
    "print(f\"  Partitions: {ddf.npartitions}\")\n",
    "print(f\"  Columns: {len(ddf.columns)}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(ddf.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    COMPUTE TOTAL ROWS (for progress tracking)\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Computing total row count...\")\n",
    "with ProgressBar():\n",
    "    total_rows = len(ddf)\n",
    "    \n",
    "print(f\"\\nTotal rows in dataset: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Parsing & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    PRICE EXTRACTION FUNCTION\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def extract_price(df):\n",
    "    \"\"\"\n",
    "    Extract price with priority:\n",
    "    1. seat_salePrice (if > 0)\n",
    "    2. seat_purchasePrice (if > 0)\n",
    "    3. seat_purchasePrice + seat_tax\n",
    "    \"\"\"\n",
    "    price = df['seat_salePrice'].fillna(0).astype(float)\n",
    "    \n",
    "    # Where sale price is invalid, use purchase price\n",
    "    use_purchase = price <= 0\n",
    "    price = price.where(~use_purchase, df['seat_purchasePrice'].fillna(0))\n",
    "    \n",
    "    # Where still invalid, add tax\n",
    "    use_tax = (price <= 0) & df['seat_purchasePrice'].notna() & df['seat_tax'].notna()\n",
    "    price = price.where(~use_tax, df['seat_purchasePrice'].fillna(0) + df['seat_tax'].fillna(0))\n",
    "    \n",
    "    return price\n",
    "\n",
    "print(\"Price extraction function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    PARSE AND TRANSFORM DATA\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Parsing and transforming data...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply transformations using Dask\n",
    "# Note: We'll compute into pandas at the end for ML training\n",
    "\n",
    "def parse_partition(pdf):\n",
    "    \"\"\"\n",
    "    Parse a single partition (pandas DataFrame).\n",
    "    This function will be applied to each Dask partition.\n",
    "    \"\"\"\n",
    "    df = pdf.copy()\n",
    "    \n",
    "    # ─── Datetime Parsing ───────────────────────────────────────────────────\n",
    "    for col, new_col in [('departureTime', 'departure_time'), \n",
    "                          ('arrivalTime', 'arrival_time'),\n",
    "                          ('doj', 'journey_date')]:\n",
    "        if col in df.columns:\n",
    "            df[new_col] = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
    "    \n",
    "    if 'scraped_time' in df.columns:\n",
    "        df['scraped_time'] = pd.to_datetime(df['scraped_time'], errors='coerce', utc=True)\n",
    "    \n",
    "    # ─── Numeric Normalization ──────────────────────────────────────────────\n",
    "    if 'availableSeats' in df.columns:\n",
    "        df['available_seats'] = pd.to_numeric(df['availableSeats'], errors='coerce').fillna(0).astype(int)\n",
    "    if 'avlWindowSeats' in df.columns:\n",
    "        df['window_seats'] = pd.to_numeric(df['avlWindowSeats'], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # ─── Boolean Flags ──────────────────────────────────────────────────────\n",
    "    if 'is_AC' in df.columns:\n",
    "        df['is_AC'] = df['is_AC'].astype(bool)\n",
    "    \n",
    "    # Seat-level flags\n",
    "    for col in ['seat_is_upper', 'seat_is_ladiesSeat', 'seat_is_horizontal', \n",
    "                'seat_is_seater', 'seat_is_available']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(False).astype(bool)\n",
    "        else:\n",
    "            df[col] = False\n",
    "    \n",
    "    # Rename seat_is_ladiesSeat for consistency\n",
    "    if 'seat_is_ladiesSeat' in df.columns:\n",
    "        df['seat_is_ladies'] = df['seat_is_ladiesSeat']\n",
    "    \n",
    "    # ─── Price Extraction ───────────────────────────────────────────────────\n",
    "    df['price'] = extract_price(df)\n",
    "    \n",
    "    # ─── Column Renaming ────────────────────────────────────────────────────\n",
    "    rename_map = {\n",
    "        'id': 'bus_id',\n",
    "        'providerId': 'provider_id',\n",
    "        'travels': 'operator_name',\n",
    "        'busType': 'bus_type',\n",
    "        'name': 'seat_name'\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply parsing to all partitions\n",
    "print(\"Applying transformations to all partitions...\")\n",
    "ddf_parsed = ddf.map_partitions(parse_partition)\n",
    "\n",
    "print(\"Transformation pipeline defined (lazy).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    SELECT FINAL COLUMNS & COMPUTE\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Define columns we need\n",
    "final_columns = [\n",
    "    'bus_id', 'provider_id', 'scraped_time', 'journey_date',\n",
    "    'departure_time', 'arrival_time', 'source_collection',\n",
    "    'bus_type', 'is_AC', 'available_seats', 'window_seats',\n",
    "    'operator_name', 'seat_name', 'seat_is_upper', 'seat_is_ladies',\n",
    "    'seat_is_horizontal', 'seat_is_seater', 'seat_is_available', 'price'\n",
    "]\n",
    "\n",
    "print(\"Computing full dataset into memory...\")\n",
    "print(\"(This may take a few minutes for large datasets)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with ProgressBar():\n",
    "    # Compute to pandas DataFrame\n",
    "    df_raw = ddf_parsed.compute()\n",
    "\n",
    "# Select only columns that exist\n",
    "existing_cols = [c for c in final_columns if c in df_raw.columns]\n",
    "df_raw = df_raw[existing_cols].copy()\n",
    "\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"  Shape: {df_raw.shape}\")\n",
    "print(f\"  Memory: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  Columns: {df_raw.columns.tolist()}\")\n",
    "\n",
    "# Clean up Dask objects\n",
    "del ddf, ddf_parsed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    PREPROCESSING CLASS\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class BusSeatPreprocessor:\n",
    "    def __init__(self, min_price=100, max_price=10000, outlier_method='iqr_1.5'):\n",
    "        self.min_price = min_price\n",
    "        self.max_price = max_price\n",
    "        self.outlier_method = outlier_method\n",
    "        self.stats = {}\n",
    "    \n",
    "    def remove_duplicates(self, df):\n",
    "        n = len(df)\n",
    "        subset_cols = ['bus_id', 'seat_name', 'journey_date', 'scraped_time']\n",
    "        subset_cols = [c for c in subset_cols if c in df.columns]\n",
    "        if subset_cols:\n",
    "            df = df.drop_duplicates(subset=subset_cols)\n",
    "        self.stats['duplicates_removed'] = n - len(df)\n",
    "        return df\n",
    "    \n",
    "    def clean_invalid(self, df):\n",
    "        n = len(df)\n",
    "        \n",
    "        # Valid seats\n",
    "        if 'available_seats' in df.columns and 'window_seats' in df.columns:\n",
    "            df = df[(df['available_seats'] >= 0) & (df['window_seats'] >= 0)]\n",
    "            df = df[df['window_seats'] <= df['available_seats']]\n",
    "        \n",
    "        # Valid price range\n",
    "        df = df[df['price'].between(self.min_price, self.max_price)]\n",
    "        \n",
    "        self.stats['invalid_removed'] = n - len(df)\n",
    "        return df\n",
    "    \n",
    "    def remove_outliers(self, df):\n",
    "        n = len(df)\n",
    "        \n",
    "        if self.outlier_method == 'iqr_1.5':\n",
    "            Q1, Q3 = df['price'].quantile([0.25, 0.75])\n",
    "            IQR = Q3 - Q1\n",
    "            lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "            df = df[(df['price'] >= lower) & (df['price'] <= upper)]\n",
    "        elif self.outlier_method == 'iqr_3':\n",
    "            Q1, Q3 = df['price'].quantile([0.25, 0.75])\n",
    "            IQR = Q3 - Q1\n",
    "            lower, upper = Q1 - 3 * IQR, Q3 + 3 * IQR\n",
    "            df = df[(df['price'] >= lower) & (df['price'] <= upper)]\n",
    "        \n",
    "        self.stats['outliers_removed'] = n - len(df)\n",
    "        return df\n",
    "    \n",
    "    def add_time_features(self, df):\n",
    "        \"\"\"Add time-based features.\"\"\"\n",
    "        \n",
    "        # Hours to departure\n",
    "        if all(c in df.columns for c in ['departure_time', 'scraped_time']):\n",
    "            df['hours_to_departure'] = (\n",
    "                (df['departure_time'] - df['scraped_time'])\n",
    "                .dt.total_seconds() / 3600\n",
    "            ).clip(lower=0)\n",
    "        \n",
    "        # Trip duration\n",
    "        if all(c in df.columns for c in ['arrival_time', 'departure_time']):\n",
    "            df['duration_hours'] = (\n",
    "                (df['arrival_time'] - df['departure_time'])\n",
    "                .dt.total_seconds() / 3600\n",
    "            ).clip(lower=0)\n",
    "        \n",
    "        # Days to journey\n",
    "        if all(c in df.columns for c in ['journey_date', 'scraped_time']):\n",
    "            df['days_to_journey'] = (\n",
    "                (df['journey_date'].dt.floor('D') - df['scraped_time'].dt.floor('D'))\n",
    "                .dt.days\n",
    "            ).clip(lower=0)\n",
    "        \n",
    "        # Time extractions\n",
    "        if 'scraped_time' in df.columns:\n",
    "            df['scrape_hour'] = df['scraped_time'].dt.hour\n",
    "        \n",
    "        if 'journey_date' in df.columns:\n",
    "            df['journey_weekday'] = df['journey_date'].dt.weekday\n",
    "        \n",
    "        if 'departure_time' in df.columns:\n",
    "            df['departure_hour'] = df['departure_time'].dt.hour\n",
    "        \n",
    "        # Log price (target)\n",
    "        df['price_log'] = np.log1p(df['price'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def run(self, df):\n",
    "        n_start = len(df)\n",
    "        print(f\"Starting preprocessing with {n_start:,} rows\")\n",
    "        \n",
    "        df = self.remove_duplicates(df)\n",
    "        df = self.clean_invalid(df)\n",
    "        df = self.remove_outliers(df)\n",
    "        df = self.add_time_features(df)\n",
    "        \n",
    "        n_end = len(df)\n",
    "        self.stats['retention_pct'] = round(n_end / n_start * 100, 1)\n",
    "        \n",
    "        print(f\"\\nPreprocessing Summary:\")\n",
    "        for k, v in self.stats.items():\n",
    "            print(f\"  {k.replace('_', ' ').title():<25}: {v}\")\n",
    "        print(f\"  Final rows: {n_end:,}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"Preprocessor class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    EXECUTE PREPROCESSING\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "preprocessor = BusSeatPreprocessor(\n",
    "    min_price=CONFIG['min_price'],\n",
    "    max_price=CONFIG['max_price'],\n",
    "    outlier_method=CONFIG['outlier_method']\n",
    ")\n",
    "\n",
    "df_clean = preprocessor.run(df_raw.copy())\n",
    "\n",
    "print(f\"\\nAfter preprocessing: {df_clean.shape}\")\n",
    "display(df_clean.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    FEATURE ENGINEERING CLASS\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class BusPriceFeatureEngineer:\n",
    "    def __init__(self):\n",
    "        self.new_features = []\n",
    "    \n",
    "    def create_features(self, df):\n",
    "        df = df.copy()\n",
    "        original_cols = set(df.columns)\n",
    "        \n",
    "        print(f\"Starting Feature Engineering with shape: {df.shape}\")\n",
    "        \n",
    "        # ─── Temporal Patterns ──────────────────────────────────────────────\n",
    "        if 'journey_weekday' in df.columns:\n",
    "            df['journey_is_weekend'] = df['journey_weekday'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        if 'departure_hour' in df.columns:\n",
    "            # Night departure (8 PM - 5 AM)\n",
    "            df['is_night_departure'] = ((df['departure_hour'] >= 20) | (df['departure_hour'] <= 5)).astype(int)\n",
    "            # Peak hours (6-9 AM, 5-8 PM)\n",
    "            df['is_peak_hour'] = (df['departure_hour'].isin([6,7,8,9,17,18,19,20])).astype(int)\n",
    "        \n",
    "        # ─── Booking Window ─────────────────────────────────────────────────\n",
    "        if 'hours_to_departure' in df.columns:\n",
    "            df['is_last_minute'] = (df['hours_to_departure'] <= 6).astype(int)\n",
    "            df['is_advance_booking'] = (df['hours_to_departure'] >= 168).astype(int)  # 7+ days\n",
    "        \n",
    "        # ─── Demand & Scarcity Signals ──────────────────────────────────────\n",
    "        if 'available_seats' in df.columns:\n",
    "            df['low_availability'] = (df['available_seats'] <= 5).astype(int)\n",
    "            df['very_low_availability'] = (df['available_seats'] <= 2).astype(int)\n",
    "            df['seats_sold_ratio'] = (1 - (df['available_seats'] / 50).clip(upper=1))\n",
    "        \n",
    "        # ─── Seat Characteristics ───────────────────────────────────────────\n",
    "        if 'seat_is_upper' in df.columns:\n",
    "            df['is_lower_berth'] = (~df['seat_is_upper']).astype(int)\n",
    "        \n",
    "        if 'window_seats' in df.columns and 'seat_is_upper' in df.columns:\n",
    "            df['is_premium_seat'] = ((~df['seat_is_upper']) & (df['window_seats'] > 0)).astype(int)\n",
    "        \n",
    "        # ─── Bus Type Features ──────────────────────────────────────────────\n",
    "        if 'bus_type' in df.columns:\n",
    "            bus_type_lower = df['bus_type'].str.lower().fillna('')\n",
    "            df['is_volvo'] = bus_type_lower.str.contains('volvo').astype(int)\n",
    "            df['is_sleeper'] = bus_type_lower.str.contains('sleeper').astype(int)\n",
    "            df['is_seater'] = bus_type_lower.str.contains('seater').astype(int)\n",
    "            df['is_multi_axle'] = bus_type_lower.str.contains('multi|axle').astype(int)\n",
    "        \n",
    "        self.new_features = sorted(set(df.columns) - original_cols)\n",
    "        print(f\"Feature Engineering complete. Shape: {df.shape}\")\n",
    "        print(f\"New features added: {self.new_features}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"Feature Engineer class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    EXECUTE FEATURE ENGINEERING\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "fe = BusPriceFeatureEngineer()\n",
    "df_features = fe.create_features(df_clean.copy())\n",
    "\n",
    "print(f\"\\nAfter Feature Engineering: {df_features.shape}\")\n",
    "display(df_features.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    CATEGORICAL ENCODING\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class CategoricalEncoder:\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "        self.encoded_columns = []\n",
    "    \n",
    "    def encode(self, df):\n",
    "        df = df.copy()\n",
    "        print(f\"Starting categorical encoding with shape: {df.shape}\")\n",
    "        \n",
    "        # High cardinality columns to encode\n",
    "        high_card_cols = ['operator_name', 'bus_type', 'source_collection', 'seat_name']\n",
    "        \n",
    "        for col in high_card_cols:\n",
    "            if col in df.columns:\n",
    "                le = LabelEncoder()\n",
    "                encoded_col = f'{col}_le'\n",
    "                \n",
    "                # Convert to string and encode\n",
    "                df[encoded_col] = le.fit_transform(df[col].astype(str))\n",
    "                \n",
    "                self.encoders[col] = le\n",
    "                self.encoded_columns.append(encoded_col)\n",
    "                print(f\"  Encoded: {col} → {encoded_col} ({len(le.classes_)} classes)\")\n",
    "        \n",
    "        print(f\"\\nEncoding complete. Shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "# Execute encoding\n",
    "encoder = CategoricalEncoder()\n",
    "df_encoded = encoder.encode(df_features.copy())\n",
    "\n",
    "print(f\"\\nEncoded columns: {encoder.encoded_columns}\")\n",
    "display(df_encoded.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Train-Test Split (GLOBAL SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    PREPARE FEATURES AND TARGET\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Preparing features and target...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define target\n",
    "target_col = 'price_log'\n",
    "\n",
    "# Columns to exclude from features\n",
    "exclude_cols = [\n",
    "    'price', 'price_log', 'price_capped',  # Target related\n",
    "    'departure_time', 'arrival_time', 'journey_date', 'scraped_time',  # Datetime objects\n",
    "    'bus_id', 'provider_id', 'seat_name',  # IDs (use encoded versions)\n",
    "    'operator_name', 'bus_type', 'source_collection',  # Categoricals (use encoded versions)\n",
    "    'hours_to_departure_bin'  # Categorical bin\n",
    "]\n",
    "\n",
    "# Select numeric features only\n",
    "numeric_cols = df_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "\n",
    "print(f\"Target: {target_col}\")\n",
    "print(f\"Features ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "# Create X and y\n",
    "X = df_encoded[feature_cols].copy()\n",
    "y = df_encoded[target_col].copy()\n",
    "\n",
    "# Handle any remaining NaN/inf values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"NaN in X: {X.isna().sum().sum()}\")\n",
    "print(f\"NaN in y: {y.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#         CRITICAL: GLOBAL SHUFFLE + TRAIN-TEST SPLIT\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Performing GLOBAL shuffle and train-test split...\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test size: {CONFIG['test_size']}\")\n",
    "print(f\"Shuffle: {CONFIG['shuffle']}\")\n",
    "print(f\"Random state: {CONFIG['random_state']}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=CONFIG['test_size'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    shuffle=CONFIG['shuffle']  # CRITICAL: Global shuffle!\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit Results:\")\n",
    "print(f\"  Training set: {X_train.shape[0]:,} samples ({100-CONFIG['test_size']*100:.0f}%)\")\n",
    "print(f\"  Test set:     {X_test.shape[0]:,} samples ({CONFIG['test_size']*100:.0f}%)\")\n",
    "print(f\"  Features:     {X_train.shape[1]}\")\n",
    "\n",
    "# Verify distributions are similar (important check!)\n",
    "print(f\"\\nTarget Distribution Check:\")\n",
    "print(f\"  Train mean: {y_train.mean():.4f}, std: {y_train.std():.4f}\")\n",
    "print(f\"  Test mean:  {y_test.mean():.4f}, std: {y_test.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    MODEL TRAINER CLASS\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, random_state=42):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.models = {}\n",
    "        self.scores = {}\n",
    "        self.best_model_name = None\n",
    "        self.best_model = None\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def _evaluate_model(self, model, name):\n",
    "        \"\"\"Evaluate model and store metrics.\"\"\"\n",
    "        y_pred_log = model.predict(self.X_test)\n",
    "        \n",
    "        # Convert from log scale to original scale for INR metrics\n",
    "        y_pred_inr = np.expm1(y_pred_log)\n",
    "        y_test_inr = np.expm1(self.y_test)\n",
    "        \n",
    "        self.scores[name] = {\n",
    "            'rmse_log': mean_squared_error(self.y_test, y_pred_log, squared=False),\n",
    "            'mae_log': mean_absolute_error(self.y_test, y_pred_log),\n",
    "            'r2_log': r2_score(self.y_test, y_pred_log),\n",
    "            'rmse_inr': mean_squared_error(y_test_inr, y_pred_inr, squared=False),\n",
    "            'mae_inr': mean_absolute_error(y_test_inr, y_pred_inr),\n",
    "            'mape_pct': mean_absolute_percentage_error(y_test_inr, y_pred_inr) * 100\n",
    "        }\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"{name} Test Metrics → \"\n",
    "            f\"RMSE(₹): {self.scores[name]['rmse_inr']:.0f} | \"\n",
    "            f\"MAPE: {self.scores[name]['mape_pct']:.2f}% | \"\n",
    "            f\"R²: {self.scores[name]['r2_log']:.4f}\"\n",
    "        )\n",
    "    \n",
    "    def train_lightgbm(self):\n",
    "        \"\"\"Train LightGBM model.\"\"\"\n",
    "        self.logger.info(\"Training LightGBM...\")\n",
    "        \n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=8,\n",
    "            num_leaves=64,\n",
    "            min_child_samples=20,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            eval_set=[(self.X_test, self.y_test)],\n",
    "            callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        self.models['LightGBM'] = model\n",
    "        self._evaluate_model(model, 'LightGBM')\n",
    "    \n",
    "    def train_xgboost(self):\n",
    "        \"\"\"Train XGBoost model.\"\"\"\n",
    "        self.logger.info(\"Training XGBoost...\")\n",
    "        \n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=8,\n",
    "            min_child_weight=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            eval_set=[(self.X_test, self.y_test)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        self.models['XGBoost'] = model\n",
    "        self._evaluate_model(model, 'XGBoost')\n",
    "    \n",
    "    def train_all(self, models_to_train=None):\n",
    "        \"\"\"Train all specified models.\"\"\"\n",
    "        if models_to_train is None:\n",
    "            models_to_train = ['LightGBM', 'XGBoost']\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"MODEL TRAINING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if 'LightGBM' in models_to_train:\n",
    "            self.train_lightgbm()\n",
    "        \n",
    "        if 'XGBoost' in models_to_train:\n",
    "            self.train_xgboost()\n",
    "        \n",
    "        # Find best model (lowest RMSE in INR)\n",
    "        if self.scores:\n",
    "            scores_df = pd.DataFrame(self.scores).T\n",
    "            self.best_model_name = scores_df['rmse_inr'].idxmin()\n",
    "            self.best_model = self.models[self.best_model_name]\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"MODEL COMPARISON\")\n",
    "            print(\"=\"*80)\n",
    "            display(scores_df.round(4))\n",
    "            \n",
    "            print(f\"\\nBEST MODEL: {self.best_model_name}\")\n",
    "            print(f\"  → RMSE (₹): {self.scores[self.best_model_name]['rmse_inr']:.0f}\")\n",
    "            print(f\"  → MAE (₹):  {self.scores[self.best_model_name]['mae_inr']:.0f}\")\n",
    "            print(f\"  → MAPE:     {self.scores[self.best_model_name]['mape_pct']:.2f}%\")\n",
    "            print(f\"  → R² (log): {self.scores[self.best_model_name]['r2_log']:.4f}\")\n",
    "\n",
    "print(\"ModelTrainer class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    TRAIN MODELS\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    random_state=CONFIG['random_state']\n",
    ")\n",
    "\n",
    "trainer.train_all(models_to_train=CONFIG['models_to_train'])\n",
    "\n",
    "# Export for later use\n",
    "best_model_name = trainer.best_model_name\n",
    "best_model = trainer.best_model\n",
    "\n",
    "print(f\"\\nExported: best_model_name = '{best_model_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Model Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    DETAILED EVALUATION\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"DETAILED EVALUATION: {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_log = best_model.predict(X_test)\n",
    "y_pred_inr = np.expm1(y_pred_log)\n",
    "y_test_inr = np.expm1(y_test)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'R² (log scale)': r2_score(y_test, y_pred_log),\n",
    "    'RMSE (₹)': mean_squared_error(y_test_inr, y_pred_inr, squared=False),\n",
    "    'MAE (₹)': mean_absolute_error(y_test_inr, y_pred_inr),\n",
    "    'MAPE (%)': mean_absolute_percentage_error(y_test_inr, y_pred_inr) * 100\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Model Metrics:\")\n",
    "for name, value in metrics.items():\n",
    "    print(f\"  {name}: {value:.4f}\")\n",
    "\n",
    "# R² interpretation\n",
    "r2 = metrics['R² (log scale)']\n",
    "if r2 >= 0.8:\n",
    "    print(f\"\\n✅ Excellent R² of {r2:.4f}! Model explains {r2*100:.1f}% of variance.\")\n",
    "elif r2 >= 0.7:\n",
    "    print(f\"\\n✅ Good R² of {r2:.4f}! Model explains {r2*100:.1f}% of variance.\")\n",
    "elif r2 >= 0.5:\n",
    "    print(f\"\\n⚠️ Moderate R² of {r2:.4f}. Model explains {r2*100:.1f}% of variance.\")\n",
    "else:\n",
    "    print(f\"\\n❌ Low R² of {r2:.4f}. Model needs improvement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    VISUALIZATION\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Actual vs Predicted (log scale)\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(y_test, y_pred_log, alpha=0.3, s=5)\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax.set_xlabel('Actual (log price)')\n",
    "ax.set_ylabel('Predicted (log price)')\n",
    "ax.set_title(f'{best_model_name} - Actual vs Predicted (log scale)\\nR² = {r2:.4f}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Actual vs Predicted (INR scale)\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(y_test_inr, y_pred_inr, alpha=0.3, s=5)\n",
    "ax.plot([y_test_inr.min(), y_test_inr.max()], [y_test_inr.min(), y_test_inr.max()], 'r--', lw=2)\n",
    "ax.set_xlabel('Actual Price (₹)')\n",
    "ax.set_ylabel('Predicted Price (₹)')\n",
    "ax.set_title(f'{best_model_name} - Actual vs Predicted (₹)\\nRMSE = ₹{metrics[\"RMSE (₹)\"]:.0f}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residual Plot\n",
    "ax = axes[1, 0]\n",
    "residuals = y_test_inr - y_pred_inr\n",
    "ax.scatter(y_pred_inr, residuals, alpha=0.3, s=5)\n",
    "ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax.set_xlabel('Predicted Price (₹)')\n",
    "ax.set_ylabel('Residual (₹)')\n",
    "ax.set_title(f'{best_model_name} - Residual Plot')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residual Distribution\n",
    "ax = axes[1, 1]\n",
    "ax.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "ax.set_xlabel('Residual (₹)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f'{best_model_name} - Residual Distribution\\nMean: ₹{residuals.mean():.0f}, Std: ₹{residuals.std():.0f}')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    FEATURE IMPORTANCE ANALYSIS\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"FEATURE IMPORTANCE - {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importance\n",
    "if best_model_name == 'LightGBM':\n",
    "    importance = best_model.feature_importances_\n",
    "elif best_model_name == 'XGBoost':\n",
    "    importance = best_model.feature_importances_\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "print(importance_df.head(20).to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "top_n = 15\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = importance_df.head(top_n)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title(f'Top {top_n} Feature Importances - {best_model_name}')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    SAVE MODEL AND ARTIFACTS\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL PERSISTENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Save best model\n",
    "model_filename = os.path.join(CONFIG['save_dir'], f'bus_price_model_{best_model_name}_DASK_{timestamp}.pkl')\n",
    "joblib.dump(best_model, model_filename)\n",
    "print(f\"Saved model: {model_filename}\")\n",
    "\n",
    "# 2. Save categorical encoders\n",
    "encoder_filename = os.path.join(CONFIG['save_dir'], f'categorical_encoder_DASK_{timestamp}.pkl')\n",
    "joblib.dump(encoder.encoders, encoder_filename)\n",
    "print(f\"Saved encoders: {encoder_filename}\")\n",
    "\n",
    "# 3. Save feature names\n",
    "features_filename = os.path.join(CONFIG['save_dir'], f'feature_names_DASK_{timestamp}.json')\n",
    "with open(features_filename, 'w') as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "print(f\"Saved {len(feature_cols)} feature names: {features_filename}\")\n",
    "\n",
    "# 4. Save feature engineer\n",
    "fe_filename = os.path.join(CONFIG['save_dir'], f'feature_engineer_DASK_{timestamp}.pkl')\n",
    "joblib.dump(fe, fe_filename)\n",
    "print(f\"Saved feature engineer: {fe_filename}\")\n",
    "\n",
    "# 5. Save metrics\n",
    "metrics_filename = os.path.join(CONFIG['save_dir'], f'model_metrics_DASK_{timestamp}.json')\n",
    "metrics_data = {\n",
    "    'model_name': best_model_name,\n",
    "    'timestamp': timestamp,\n",
    "    'approach': 'DASK_FULL_SHUFFLE',\n",
    "    'metrics': {\n",
    "        'r2_log': float(metrics['R² (log scale)']),\n",
    "        'rmse_inr': float(metrics['RMSE (₹)']),\n",
    "        'mae_inr': float(metrics['MAE (₹)']),\n",
    "        'mape_pct': float(metrics['MAPE (%)'])\n",
    "    },\n",
    "    'training_samples': int(len(X_train)),\n",
    "    'testing_samples': int(len(X_test)),\n",
    "    'num_features': len(feature_cols),\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "with open(metrics_filename, 'w') as f:\n",
    "    json.dump(metrics_data, f, indent=2, default=str)\n",
    "print(f\"Saved metrics: {metrics_filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ MODEL PERSISTENCE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFiles saved in: {os.path.abspath(CONFIG['save_dir'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    FINAL SUMMARY\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"                    TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"═\"*80)\n",
    "\n",
    "print(f\"\\n📊 Dataset:\")\n",
    "print(f\"   Total samples processed: {len(df_encoded):,}\")\n",
    "print(f\"   Training samples: {len(X_train):,}\")\n",
    "print(f\"   Test samples: {len(X_test):,}\")\n",
    "print(f\"   Features used: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "print(f\"   R² (log scale):  {metrics['R² (log scale)']:.4f}\")\n",
    "print(f\"   RMSE (₹):        {metrics['RMSE (₹)']:.0f}\")\n",
    "print(f\"   MAE (₹):         {metrics['MAE (₹)']:.0f}\")\n",
    "print(f\"   MAPE (%):        {metrics['MAPE (%)']:.2f}%\")\n",
    "\n",
    "print(f\"\\n💾 Saved Artifacts:\")\n",
    "print(f\"   Model: {model_filename}\")\n",
    "print(f\"   Encoders: {encoder_filename}\")\n",
    "print(f\"   Features: {features_filename}\")\n",
    "print(f\"   Metrics: {metrics_filename}\")\n",
    "\n",
    "print(\"\\n\" + \"═\"*80)\n",
    "print(\"                    ✅ READY FOR PRODUCTION\")\n",
    "print(\"═\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14: Sample Prediction (Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "#                    SAMPLE PREDICTION\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"Sample Predictions on Test Data:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get some sample predictions\n",
    "sample_idx = np.random.choice(len(X_test), size=10, replace=False)\n",
    "X_sample = X_test.iloc[sample_idx]\n",
    "y_sample_actual = np.expm1(y_test.iloc[sample_idx])\n",
    "y_sample_pred = np.expm1(best_model.predict(X_sample))\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'Actual (₹)': y_sample_actual.values,\n",
    "    'Predicted (₹)': y_sample_pred,\n",
    "    'Difference (₹)': y_sample_actual.values - y_sample_pred,\n",
    "    'Error (%)': abs(y_sample_actual.values - y_sample_pred) / y_sample_actual.values * 100\n",
    "})\n",
    "\n",
    "print(comparison.round(2).to_string(index=False))\n",
    "\n",
    "print(f\"\\nAverage absolute error: ₹{abs(comparison['Difference (₹)']).mean():.0f}\")\n",
    "print(f\"Average percentage error: {comparison['Error (%)'].mean():.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
